\documentclass[11pt]{article}
\usepackage{graphicx, subcaption, amsfonts, amsmath}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\graphicspath{ {./figs/} }
\pagestyle{plain}
\begin{document}
\title{Overview of techniques used in quantifying graph similarity}
\author{Alexander Holiday}
\date{}
\maketitle

In the field of machine learning, determining the degree of similarity between two inputs is a necessary aspect of any algorithm. Often the data can be well analyzed by fast, exisiting distance measures, e.g. the Levenshtein distance for strings, or the Euclidian distance for vectors in $\mathbb{R}^{n}$. However, when confronted with inputs in the form of graphs, special difficulties arise, mainly from the exponential complexity associated with calculating many of the ``natural'' notions of graph similarity. Three main branches of research have been established around this problem: graph edit distances, graph kernels, and graph embeddings. Each will be reviewed below.

\section{Graph edit distances}
\subsection{Theoretical motivation}
The methods contained within this section can be thought of as generalizations of a well-known metric on the space of graphs, based on either the maximum common subgraph or, equivalently, the minimal common supergraph. The maximum common subgraph of $G_1$ and $G_2$ is the largest subgraph of $G_1$ isomorphic to some part of $G_2$, while the minimum common supergraph is the smallest graph that contains $G_1$ and $G_2$ as subgraphs. Given two graphs, $G_1$ and $G_2$, we define the distance between them as

\begin{align}
d_{subraph}(G_1, G_2) = 1 - \frac{|mcs(G_1, G_2)|}{max(|G_1|, |G_2|)} = 1 - \frac{|G_1| + |G_2| + |MCS(G_1, G_2)|}{max(|G_1|, |G_2|)}
\label{eqn:mcs}
\end{align}

where $mcs(G_1, G_2)$ denotes the maximum common subgraph, and $MCS(G_1, G_2)$ the minimum common supergraph. These expressions are provably metrics on the space of graphs, but the calculation of either $mcs(G_1, G_2)$ or $MCS(G_1, G_2)$ is NP-hard.

\subsection{Edit distance algorithms}

Preliminarily, we define an \textit{edit operation}, $o$, as an insertion, deletion or subsitution of either a vertex or edge. To each type of operation, we assign some cost $c(o)$. Given $G_1$ and $G_2$, there are many different sequences of edit operations that will transform $G_1$ into $G_2$. We will call this set of all sequences $\mathcal{O}$ and define the \textit{graph edit distance} to be the sequence of minimum cost, i.e.

\begin{align}
d_{GED}(G_1, G_2) = \min\limits_{o \in \mathcal{O}} \sum\limits_{i} c(o_i)
\label{eqn:ged}
\end{align}
 
This optimization problem is known to be, in general, exponential in the number of vertices involved; thus, many approximation algorithms have been proposed. An exact algorithm, based on an $A^{*}$ search, creates a tree enumerating possible edit paths. At each step, the search is continued along the node with the current minimum total cost, i.e. the cost of traversing the tree from root to the current node. An approximation, the $A^{*} Beamsearch$ leaves only the $s$ most promising nodes in the search path. An alternative approach to these $A^{*}$-based algorithms is known as \textit{fuzzy edit distance}, where edit operations are not explicitly included or excluded but rather assigned some weight. This method begins by producing a matrix $\textbf{Q}^{n_1n_2 \times n_1n_2}$ where $n = |G|$. The rows and columns are indexed by vertex substitutions from $V_1$ to $V_2$; thus the diagonal entries, of the form $(u \rightarrow v, u \rightarrow v)$, correspond to the cost of substituting vertex $v \in V_2$ for $u \in V_1$. Then we must solve

\begin{align}
x_{fuzzy} = \min\limits_{x} x^T\textbf{Q}x
\label{eqn:fuzzy}
\end{align}

This gives us a vector of weights $x$, which is then processed through some \textit{defuzzification} to yield the actual edit path and distance. 

There are still other formulations of edit cost algorithms. The popularity of this measure of similarity lies in its generality and robustness to errors. The cost functions can be defined to take into account information specific to a certain field, and indeed completely new edit operations can be created if desired.

\subsection{Applications}

This framework has been applied in a number of machine learning areas, including object recognition and video indexing. %\cite{edit distance refs}

\section{Graph kernels}
\subsection{Graph kernel variations}
This class of similarity measures generally compare random walks over graphs, though the simplest formulation, the \textit{direct product kernel}, takes the form

\begin{align}
k(G_1, G_2) = \sum\limits_{i,j}[\sum\limits_{n=0}^{\infty}\lambda_n A_\times^n]_{ij}
\label{eqn:dpk}
\end{align}

and thereby compares directly the number of paths shared by the input graphs. Here, the $\times$ represents a direct product value; thus, $A_\times$ is the direct product adjacency matrix, formed by ``inserting'' $A_2$ into every entry of $A_1$. 

The more common random walk kernel is given by 

\begin{align}
k(G_1, G_2) = \sum\limits_{k=1}^{\infty}\mu(k) q_\times^T W_\times^k p_\times
\label{eqn:gk}
\end{align}


in which $p_\times$ and $q_\times$ are starting and stopping probabilities, respectively. (\ref{eqn:gk}) can be simplified with the proper choice of $\mu(k)$, e.g. $\mu(k) = \frac{\lambda^k}{k!}$ leads to $k(G_1, G_2) = q_\times^T e^{W_\times \lambda} p_\times$. \\
The power in this formulation is that $W_\times$ is defined very generally as $W_\times = \Phi(L_1)\otimes\Phi(L_2)$ where $\Phi()$, loosely speaking, takes an edge label and assigns it a value in $\mathbb{R}$, and $L$ is a matrix of edge labels. Thus, for simplicity, we could define $\Phi(L) = A$, yielding $W_\times = A_\times$. An extension to labeled graphs could be accomplished by the following definitions, given a set of edge labels $\mathcal{L} = {l_1,...,l_d}$

\begin{equation}
\Phi(L_{ij}) = \left \{
\begin{array}{l l}
\frac{e_i}{deg(v_i)} & \text{if the edge $e_{ij} \in E_\times$ and $label(e_{ij}) = l_i$} \\
0 & \text{otherwise}
\end{array}
\right.
\label{eqn:lgk} 
\end{equation}

This would result in $W_\times$ having nonzero entries only when the corresponding edges exist in $G_\times$ and the labels are the same as those in the original graphs. \\
As these kernel computations often amount to some eigendecomposition, efficient algorithms for a number of cases have been presented.

\subsection{Applications}

Graph kernels have been applied to problems in chemical toxicity, protein function prediction, and microarray analysis. Results for protein function prediction are said to be ``state-of-the-art'', while other application areas appear to need more tuning to obtain useful results.

\section{Graph embedding}

This strategy maps each input graph to a vector in $\mathbb{R}^m$. Two possible approaches are to compare input graphs against a set of prototypes, or to count the occurrences of a set of subgraphs in each input graph. In the first, given a set $\mathcal{P} = \{P_1,...,P_m\}$ of prototype graphs, each input $G$ is mapped via

\begin{align}
\Phi(G)=[d(G,P_1), d(G,P_2),...,d(G,P_m)]
\end{align}

where $d(G, P)$ is some distance function, often the graph edit distance. Alternatively, given a set $\mathcal{S} = \{S_1,...,S_m\}$ of subgraphs, each graph can be embedded as

\begin{align}
\Phi(G)=[occ(G,S_1), occ(G,S_2),...,occ(G,S_m)]
\end{align}

where $occ(G, S)$ denotes the number of occurences of subgraph $S$ in the input graph $G$. The strength of embedding methods lies in their easy incorporation into existing data mining techniques, as the output is a vector in $\mathbb{R}^m$.

\subsection{Applications}

Graph embeddings have been used in conjunction with classical scaling techniques to embed the graphs (embed the embeddings) into a lower dimensional subspace.

%% \section{Iterative methods}

%% This class of techniques is based on an iterative update of similarities of nodes, and possibly edges, between two graphs. It was pioneered for identification of ``hubs'' and ``authorities'' in the internet. In brief, considering only vertex-based methods, we start with some initial distribution of 

\bibliographystyle{abbrv}
\bibliography{$HOME/Documents/bibTex/library/}
\end{document}
